# Repository Overview

This repository contains the code for ACL2025 paper **Evaluating the Evaluation of Diversity in Commonsense Generation**

It includes generating candidate sets and evaluating diversity metrics, as well as the experimental data used in our paper.

## Data

All experimental data used in our paper are stored in the `data` folder, organized as follows:

- **`data/comparison`**  
  Contains all high-quality and low-quality sets.

- **`data/quality_evaluation`**  
  Contains the evaluation results generated by the CommonGen quality evaluation script.

- **Root of `data/`**  
  Contains the human evaluation results and the results for Table 1.

## Code

The scripts for generating candidate sets and evaluating diversity metrics are located in the `code` folder. Before you run the code, you need to install the necessary nlg-eval package from https://github.com/Maluuba/nlg-eval.

- **`llm_generator.py`**  
  Run this script to regenerate candidate sets.

- **`llm_evalutator.py`**  
  Run this script to reevaluate the data.
